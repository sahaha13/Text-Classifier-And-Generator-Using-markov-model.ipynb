{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generating poetry using Natural Language processing (Markov Principle and Bayes Theorem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7WEUIawlhQS0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import string\n",
        "\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wabDNZ_7hd3C"
      },
      "outputs": [],
      "source": [
        "initial = {} # Start of the phrase\n",
        "first_order = {} # Second word only\n",
        "second_order = {} # 2nd order transition probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qi40F3vBhrjN"
      },
      "outputs": [],
      "source": [
        "def remove_punctuations(s):\n",
        "  return s.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TExyEnbh5_d",
        "outputId": "0938359d-b88a-493b-9ce5-ae1716fdb05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-05-19 16:59:37--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56286 (55K) [text/plain]\n",
            "Saving to: ‘robert_frost.txt’\n",
            "\n",
            "\rrobert_frost.txt      0%[                    ]       0  --.-KB/s               \rrobert_frost.txt    100%[===================>]  54.97K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-05-19 16:59:37 (5.10 MB/s) - ‘robert_frost.txt’ saved [56286/56286]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M8GZH0kXh7pB"
      },
      "outputs": [],
      "source": [
        "def add2dict(d,k,v): # dictionary, key, value(list storing all possible next words)\n",
        "  if k not in d:\n",
        "    d[k] =[]\n",
        "    d[k].append(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hyFaGBdhiHJw"
      },
      "outputs": [],
      "source": [
        "for line in open('robert_frost.txt'):\n",
        "  tokens = remove_punctuations(line.rstrip().lower()).split()\n",
        "\n",
        "  T = len(tokens)\n",
        "  for i in range(T):\n",
        "    t = tokens[i]\n",
        "    if i == 0: # indicates that i is the first word in a sentence\n",
        "      # Measure disstribution of first word\n",
        "      initial[t] = initial.get(t, 0.) + 1 # get method to ensure that count is 0 if t does not exist in the dictionary\n",
        "    else:\n",
        "        t_1 = tokens[i-1]\n",
        "        if i == T -1: # end of a sentence \n",
        "          # Measure probability of ending the line\n",
        "          add2dict(second_order,(t_1,t), 'END')\n",
        "        if i == 1:\n",
        "          # measure probability of second word in a sentence\n",
        "          # Given only the first word\n",
        "          add2dict(first_order,t_1,t)\n",
        "        else:\n",
        "          t_2= tokens[i-2]\n",
        "          add2dict(second_order,(t_2,t_1),t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9qxpsIJGT3Gu"
      },
      "outputs": [],
      "source": [
        "# Normalize the distribution\n",
        "initial_total = sum(initial.values()) # Only initial dict contains actual word counts,other dicts only contain probabilities of next words\n",
        "for t,c in initial.items(): # c = counts, t = tokens\n",
        "  initial[t] = c/initial_total "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xoqiU6YpUHMu"
      },
      "outputs": [],
      "source": [
        "# Create dictionary of counts for the otherdicts and normalise the counts by converting them into probabilities\n",
        "def list2pdict(ts): # ts= list of tokens\n",
        "  d={}\n",
        "  n = len(ts)\n",
        "  for t in ts:\n",
        "    d[t] = d.get(t, 0.) + 1\n",
        "  for t,c in d.items():\n",
        "    d[t] = c/n\n",
        "  return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LfF8WSKfVceL"
      },
      "outputs": [],
      "source": [
        "for t_1, ts in first_order.items():\n",
        "  first_order[t_1] = list2pdict(ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aKn-aEfXV_2P"
      },
      "outputs": [],
      "source": [
        "for k, ts in second_order.items():\n",
        "  second_order[k] = list2pdict(ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "B7lOJOlmWRng"
      },
      "outputs": [],
      "source": [
        "def sample_word(d):\n",
        "  p0 = np.random.random()\n",
        "  cumulative = 0\n",
        "  for t,p in d.items():\n",
        "    cumulative+= p\n",
        "    if p0 < cumulative:\n",
        "      return t\n",
        "  assert(False) # should never get here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "gB7Xm5knZtJS"
      },
      "outputs": [],
      "source": [
        "def generate():\n",
        "  for i in range(4):\n",
        "    sentence = []\n",
        "\n",
        "    # initial word\n",
        "    w0 = sample_word(initial)\n",
        "    sentence.append(w0)\n",
        "\n",
        "    # Sample second word\n",
        "    w1 = sample_word(first_order[w0])\n",
        "    sentence.append(w1)\n",
        "\n",
        "    # Second order transitions until the EOL\n",
        "    while True:\n",
        "      w2 = sample_word(second_order[(w0,w1)])\n",
        "      if w2== 'END':\n",
        "        break\n",
        "      sentence.append(w2)\n",
        "      w0=w1\n",
        "      w1=w2\n",
        "    print(' '.join(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06svMr93amWm",
        "outputId": "bb2478b1-3af8-4087-95ad-9d69a2eba7cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i doubted if i should ever come back\n",
            "hes celebrating something strange\n",
            "or been made master of the year\n",
            "was wrought through trees without a farmhouse near\n"
          ]
        }
      ],
      "source": [
        "generate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPaUupHPauwA"
      },
      "source": [
        "****"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Poetry-Generation-Using-Markov-Models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
